# -*- coding: utf-8 -*-
"""
æ™ºèƒ½å®¶å±…è¯­éŸ³æ•°æ®é›†ç”Ÿæˆå™¨
ç”¨äºç”Ÿæˆåˆæˆçš„æ™ºèƒ½å®¶å±…è¯­éŸ³æ§åˆ¶æ•°æ®
"""

import os
import numpy as np
import torch
import torchaudio
from typing import List, Tuple, Dict
import json
import random
from dataclasses import dataclass

@dataclass
class AudioSample:
    """éŸ³é¢‘æ ·æœ¬æ•°æ®ç±»"""
    audio: np.ndarray
    text: str
    tokens: List[int]
    class_label: int
    duration: float
    sample_rate: int

class SmartHomeSpeechSynthesizer:
    """
    æ™ºèƒ½å®¶å±…è¯­éŸ³åˆæˆå™¨
    ç”Ÿæˆæ¨¡æ‹Ÿçš„æ™ºèƒ½å®¶å±…æ§åˆ¶è¯­éŸ³æ•°æ®
    """
    
    def __init__(self, sample_rate: int = 16000):
        self.sample_rate = sample_rate
        
        # æ™ºèƒ½å®¶å±…æŒ‡ä»¤æ¨¡æ¿
        self.command_templates = [
            # ç¯å…‰æ§åˆ¶
            ("æ‰“å¼€{room}çš„ç¯", "light_on", ["æ‰“å¼€", "{room}", "çš„", "ç¯"]),
            ("å…³é—­{room}çš„ç¯", "light_off", ["å…³é—­", "{room}", "çš„", "ç¯"]),
            ("è°ƒäº®{room}çš„ç¯", "light_bright", ["è°ƒäº®", "{room}", "çš„", "ç¯"]),
            ("è°ƒæš—{room}çš„ç¯", "light_dim", ["è°ƒæš—", "{room}", "çš„", "ç¯"]),
            
            # ç©ºè°ƒæ§åˆ¶
            ("æ‰“å¼€ç©ºè°ƒ", "ac_on", ["æ‰“å¼€", "ç©ºè°ƒ"]),
            ("å…³é—­ç©ºè°ƒ", "ac_off", ["å…³é—­", "ç©ºè°ƒ"]),
            ("è°ƒé«˜æ¸©åº¦", "ac_temp_up", ["è°ƒé«˜", "æ¸©åº¦"]),
            ("è°ƒä½æ¸©åº¦", "ac_temp_down", ["è°ƒä½", "æ¸©åº¦"]),
            ("è®¾ç½®æ¸©åº¦ä¸º{temp}åº¦", "ac_temp_set", ["è®¾ç½®", "æ¸©åº¦", "ä¸º", "{temp}", "åº¦"]),
            
            # ç”µè§†æ§åˆ¶
            ("æ‰“å¼€ç”µè§†", "tv_on", ["æ‰“å¼€", "ç”µè§†"]),
            ("å…³é—­ç”µè§†", "tv_off", ["å…³é—­", "ç”µè§†"]),
            ("è°ƒé«˜éŸ³é‡", "tv_volume_up", ["è°ƒé«˜", "éŸ³é‡"]),
            ("è°ƒä½éŸ³é‡", "tv_volume_down", ["è°ƒä½", "éŸ³é‡"]),
            
            # çª—å¸˜æ§åˆ¶
            ("æ‰“å¼€çª—å¸˜", "curtain_open", ["æ‰“å¼€", "çª—å¸˜"]),
            ("å…³é—­çª—å¸˜", "curtain_close", ["å…³é—­", "çª—å¸˜"]),
            ("æ‹‰å¼€{room}çš„çª—å¸˜", "curtain_open_room", ["æ‹‰å¼€", "{room}", "çš„", "çª—å¸˜"]),
            
            # é£æ‰‡æ§åˆ¶
            ("æ‰“å¼€é£æ‰‡", "fan_on", ["æ‰“å¼€", "é£æ‰‡"]),
            ("å…³é—­é£æ‰‡", "fan_off", ["å…³é—­", "é£æ‰‡"]),
            ("è°ƒèŠ‚é£æ‰‡é€Ÿåº¦", "fan_speed", ["è°ƒèŠ‚", "é£æ‰‡", "é€Ÿåº¦"]),
        ]
        
        # æˆ¿é—´åç§°
        self.rooms = ["å®¢å…", "å§å®¤", "å¨æˆ¿", "ä¹¦æˆ¿", "ä¸»å§", "æ¬¡å§"]
        
        # æ¸©åº¦æ•°å€¼
        self.temperatures = ["18", "20", "22", "24", "26", "28"]
        
        # è¯­éŸ³åˆæˆå‚æ•°
        self.voice_params = {
            'male': {
                'f0_range': (80, 200),
                'formants': [700, 1220, 2600],
                'noise_level': 0.05
            },
            'female': {
                'f0_range': (150, 300),
                'formants': [900, 1400, 2800],
                'noise_level': 0.04
            },
            'child': {
                'f0_range': (200, 400),
                'formants': [1000, 1600, 3000],
                'noise_level': 0.06
            }
        }
    
    def generate_command_variations(self) -> List[Tuple[str, str, List[str]]]:
        """ç”ŸæˆæŒ‡ä»¤å˜ä½“"""
        variations = []
        
        for template, intent, tokens in self.command_templates:
            if "{room}" in template:
                for room in self.rooms:
                    text = template.format(room=room)
                    new_tokens = [token.format(room=room) if token == "{room}" else token for token in tokens]
                    variations.append((text, intent, new_tokens))
            elif "{temp}" in template:
                for temp in self.temperatures:
                    text = template.format(temp=temp)
                    new_tokens = [token.format(temp=temp) if token == "{temp}" else token for token in tokens]
                    variations.append((text, intent, new_tokens))
            else:
                variations.append((template, intent, tokens))
        
        return variations
    
    def synthesize_speech(self, 
                         text: str, 
                         duration: float = None,
                         voice_type: str = 'male') -> np.ndarray:
        """
        åˆæˆè¯­éŸ³ä¿¡å·
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            duration: éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
            voice_type: å£°éŸ³ç±»å‹ ('male', 'female', 'child')
        
        Returns:
            np.ndarray: åˆæˆçš„éŸ³é¢‘ä¿¡å·
        """
        if duration is None:
            # æ ¹æ®æ–‡æœ¬é•¿åº¦ä¼°ç®—æ—¶é•¿
            duration = max(1.0, len(text) * 0.15 + np.random.uniform(0.5, 1.0))
        
        samples = int(duration * self.sample_rate)
        t = np.linspace(0, duration, samples)
        
        # è·å–å£°éŸ³å‚æ•°
        params = self.voice_params[voice_type]
        f0_min, f0_max = params['f0_range']
        formants = params['formants']
        noise_level = params['noise_level']
        
        # ç”ŸæˆåŸºé¢‘è½®å»“
        f0 = np.random.uniform(f0_min, f0_max)
        f0_contour = f0 * (1 + 0.1 * np.sin(2 * np.pi * 2 * t))  # æ·»åŠ è¯­è°ƒå˜åŒ–
        
        # ç”Ÿæˆè¯­éŸ³ä¿¡å·
        signal = np.zeros_like(t)
        
        # æ·»åŠ åŸºé¢‘å’Œè°æ³¢
        for harmonic in range(1, 6):
            amplitude = 1.0 / harmonic
            harmonic_freq = f0_contour * harmonic
            signal += amplitude * np.sin(2 * np.pi * harmonic_freq * t)
        
        # æ·»åŠ å…±æŒ¯å³°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        for i, formant_freq in enumerate(formants[:3]):
            formant_amp = 0.3 / (i + 1)
            signal += formant_amp * np.sin(2 * np.pi * formant_freq * t)
        
        # æ·»åŠ è¯­éŸ³åŒ…ç»œ
        # æ¨¡æ‹Ÿè¯­éŸ³çš„èµ·ä¼
        envelope = np.ones_like(t)
        
        # æ·»åŠ éŸ³èŠ‚è¾¹ç•Œ
        num_syllables = max(1, len(text) // 2)
        syllable_duration = duration / num_syllables
        
        for i in range(num_syllables):
            start_idx = int(i * syllable_duration * self.sample_rate)
            end_idx = int((i + 1) * syllable_duration * self.sample_rate)
            if end_idx > len(envelope):
                end_idx = len(envelope)
            
            # éŸ³èŠ‚å†…çš„åŒ…ç»œ
            syl_len = end_idx - start_idx
            if syl_len > 0:
                syl_env = 0.5 + 0.5 * np.cos(np.linspace(0, 2*np.pi, syl_len))
                envelope[start_idx:end_idx] *= syl_env
        
        # åº”ç”¨åŒ…ç»œ
        signal *= envelope
        
        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, noise_level, samples)
        signal += noise
        
        # åº”ç”¨é«˜é€šæ»¤æ³¢å™¨ï¼ˆæ¨¡æ‹Ÿéº¦å…‹é£å“åº”ï¼‰
        # ç®€åŒ–ç‰ˆï¼šä½¿ç”¨å·®åˆ†
        signal = np.diff(np.concatenate([[0], signal]))
        
        # å½’ä¸€åŒ–
        if np.max(np.abs(signal)) > 0:
            signal = signal / np.max(np.abs(signal)) * 0.8
        
        return signal.astype(np.float32)
    
    def create_dataset(self, 
                      num_samples: int = 1000,
                      voice_distribution: Dict[str, float] = None) -> List[AudioSample]:
        """
        åˆ›å»ºæ•°æ®é›†
        
        Args:
            num_samples: æ ·æœ¬æ•°é‡
            voice_distribution: å£°éŸ³ç±»å‹åˆ†å¸ƒ
        
        Returns:
            List[AudioSample]: éŸ³é¢‘æ ·æœ¬åˆ—è¡¨
        """
        if voice_distribution is None:
            voice_distribution = {'male': 0.4, 'female': 0.4, 'child': 0.2}
        
        print(f"ğŸ™ï¸ ç”Ÿæˆ {num_samples} ä¸ªè¯­éŸ³æ ·æœ¬...")
        
        # ç”Ÿæˆæ‰€æœ‰æŒ‡ä»¤å˜ä½“
        command_variations = self.generate_command_variations()
        
        samples = []
        intent_to_label = {}
        current_label = 0
        
        for i in range(num_samples):
            # éšæœºé€‰æ‹©æŒ‡ä»¤
            text, intent, tokens = random.choice(command_variations)
            
            # åˆ†é…ç±»åˆ«æ ‡ç­¾
            if intent not in intent_to_label:
                intent_to_label[intent] = current_label
                current_label += 1
            
            class_label = intent_to_label[intent]
            
            # éšæœºé€‰æ‹©å£°éŸ³ç±»å‹
            voice_type = np.random.choice(
                list(voice_distribution.keys()),
                p=list(voice_distribution.values())
            )
            
            # ç”Ÿæˆè¯­éŸ³
            duration = np.random.uniform(1.0, 3.5)
            audio = self.synthesize_speech(text, duration, voice_type)
            
            # åˆ›å»ºtoken IDåˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
            token_ids = list(range(len(tokens)))
            
            sample = AudioSample(
                audio=audio,
                text=text,
                tokens=token_ids,
                class_label=class_label,
                duration=duration,
                sample_rate=self.sample_rate
            )
            
            samples.append(sample)
            
            if (i + 1) % 100 == 0:
                print(f"  å·²ç”Ÿæˆ {i + 1}/{num_samples} ä¸ªæ ·æœ¬")
        
        print(f"âœ… æ•°æ®é›†ç”Ÿæˆå®Œæˆ! å…± {len(samples)} ä¸ªæ ·æœ¬ï¼Œ{len(intent_to_label)} ä¸ªç±»åˆ«")
        
        # ä¿å­˜ç±»åˆ«æ˜ å°„
        label_mapping = {intent: label for intent, label in intent_to_label.items()}
        
        return samples, label_mapping
    
    def save_dataset(self, 
                    samples: List[AudioSample], 
                    label_mapping: Dict,
                    save_dir: str):
        """
        ä¿å­˜æ•°æ®é›†
        
        Args:
            samples: éŸ³é¢‘æ ·æœ¬åˆ—è¡¨
            label_mapping: æ ‡ç­¾æ˜ å°„
            save_dir: ä¿å­˜ç›®å½•
        """
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"ğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ° {save_dir}...")
        
        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶å’Œæ ‡æ³¨
        audio_dir = os.path.join(save_dir, 'audio')
        os.makedirs(audio_dir, exist_ok=True)
        
        annotations = []
        
        for i, sample in enumerate(samples):
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            audio_filename = f"sample_{i:06d}.wav"
            audio_path = os.path.join(audio_dir, audio_filename)
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ä¿å­˜
            audio_tensor = torch.from_numpy(sample.audio).unsqueeze(0)
            torchaudio.save(audio_path, audio_tensor, sample.sample_rate)
            
            # æ·»åŠ æ ‡æ³¨ä¿¡æ¯
            annotation = {
                'file': audio_filename,
                'text': sample.text,
                'tokens': sample.tokens,
                'class_label': sample.class_label,
                'duration': sample.duration
            }
            annotations.append(annotation)
        
        # ä¿å­˜æ ‡æ³¨æ–‡ä»¶
        annotations_path = os.path.join(save_dir, 'annotations.json')
        with open(annotations_path, 'w', encoding='utf-8') as f:
            json.dump(annotations, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ ‡ç­¾æ˜ å°„
        mapping_path = os.path.join(save_dir, 'label_mapping.json')
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(label_mapping, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_samples': len(samples),
            'num_classes': len(label_mapping),
            'total_duration': sum(s.duration for s in samples),
            'avg_duration': np.mean([s.duration for s in samples]),
            'sample_rate': samples[0].sample_rate if samples else 16000
        }
        
        stats_path = os.path.join(save_dir, 'dataset_stats.json')
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ•°æ®é›†ä¿å­˜å®Œæˆ!")
        print(f"   éŸ³é¢‘æ–‡ä»¶: {len(samples)} ä¸ª")
        print(f"   æ€»æ—¶é•¿: {stats['total_duration']:.1f} ç§’")
        print(f"   å¹³å‡æ—¶é•¿: {stats['avg_duration']:.2f} ç§’")
        print(f"   ç±»åˆ«æ•°: {len(label_mapping)}")

def create_smart_home_dataset(num_samples: int = 1000, save_dir: str = None):
    """
    åˆ›å»ºæ™ºèƒ½å®¶å±…è¯­éŸ³æ•°æ®é›†
    
    Args:
        num_samples: æ ·æœ¬æ•°é‡
        save_dir: ä¿å­˜ç›®å½•
    """
    if save_dir is None:
        save_dir = os.path.join(os.path.dirname(__file__), 'res', 'smart_home_dataset')
    
    print("ğŸ  åˆ›å»ºæ™ºèƒ½å®¶å±…è¯­éŸ³æ•°æ®é›†")
    print("=" * 50)
    
    # åˆ›å»ºåˆæˆå™¨
    synthesizer = SmartHomeSpeechSynthesizer()
    
    # ç”Ÿæˆæ•°æ®é›†
    samples, label_mapping = synthesizer.create_dataset(
        num_samples=num_samples,
        voice_distribution={'male': 0.4, 'female': 0.4, 'child': 0.2}
    )
    
    # ä¿å­˜æ•°æ®é›†
    synthesizer.save_dataset(samples, label_mapping, save_dir)
    
    return samples, label_mapping

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ™ºèƒ½å®¶å±…è¯­éŸ³æ•°æ®é›†ç”Ÿæˆå™¨')
    parser.add_argument('--num_samples', type=int, default=1000,
                       help='ç”Ÿæˆæ ·æœ¬æ•°é‡ (é»˜è®¤: 1000)')
    parser.add_argument('--save_dir', type=str, default=None,
                       help='ä¿å­˜ç›®å½• (é»˜è®¤: res/smart_home_dataset)')
    parser.add_argument('--preview', action='store_true',
                       help='é¢„è§ˆæ¨¡å¼ï¼Œåªç”Ÿæˆå°‘é‡æ ·æœ¬')
    
    args = parser.parse_args()
    
    if args.preview:
        num_samples = 10
        print("ğŸ‘€ é¢„è§ˆæ¨¡å¼ï¼Œç”Ÿæˆ 10 ä¸ªæ ·æœ¬")
    else:
        num_samples = args.num_samples
    
    try:
        samples, label_mapping = create_smart_home_dataset(
            num_samples=num_samples,
            save_dir=args.save_dir
        )
        
        print("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"æ€»æ ·æœ¬æ•°: {len(samples)}")
        print(f"ç±»åˆ«æ•°: {len(label_mapping)}")
        print(f"ç±»åˆ«åˆ†å¸ƒ:")
        
        class_counts = {}
        for sample in samples:
            class_counts[sample.class_label] = class_counts.get(sample.class_label, 0) + 1
        
        for intent, label in sorted(label_mapping.items(), key=lambda x: x[1]):
            count = class_counts.get(label, 0)
            print(f"  {intent}: {count} ä¸ªæ ·æœ¬")
        
        print(f"\nâœ… æ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
