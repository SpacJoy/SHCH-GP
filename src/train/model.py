# -*- coding: utf-8 -*-
"""
è½»é‡çº§è¯­éŸ³è¯†åˆ«æ¨¡å‹
ä¸“ä¸ºæ™ºèƒ½å®¶å±…è¯­éŸ³æ§åˆ¶ä¼˜åŒ–çš„è½»é‡çº§æ·±åº¦å­¦ä¹ æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import librosa
import numpy as np
import os
import json
import pickle
from typing import List, Tuple, Optional, Dict
import warnings
warnings.filterwarnings("ignore")

class LightASRModel(nn.Module):
    """
    è½»é‡çº§è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹
    ä½¿ç”¨CNN+RNNæ¶æ„ï¼Œä¸“é—¨ä¼˜åŒ–ç”¨äºæ™ºèƒ½å®¶å±…è¯­éŸ³æ§åˆ¶æŒ‡ä»¤è¯†åˆ«
    """
    
    def __init__(self, 
                 vocab_size: int = 1000,
                 input_dim: int = 80,  # MFCCç‰¹å¾ç»´åº¦
                 hidden_dim: int = 256,
                 num_layers: int = 2,
                 num_classes: int = 50,  # æ™ºèƒ½å®¶å±…æŒ‡ä»¤ç±»åˆ«æ•°
                 dropout: float = 0.1):
        super(LightASRModel, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.vocab_size = vocab_size
        self.num_classes = num_classes
        
        # CNNç‰¹å¾æå–å±‚ - è½»é‡çº§è®¾è®¡
        self.conv_layers = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2, 2)),
            
            nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=(2, 2)),
            
            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, None))  # å…¨å±€å¹³å‡æ± åŒ–
        )
        
        # RNNåºåˆ—å»ºæ¨¡å±‚
        self.rnn = nn.LSTM(
            input_size=128,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ - è½»é‡çº§
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1),
            nn.Softmax(dim=1)
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
        
        # CTCè§£ç å™¨ï¼ˆç”¨äºåºåˆ—åˆ°åºåˆ—ï¼‰
        self.ctc_layer = nn.Linear(hidden_dim * 2, vocab_size)
        
    def forward(self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, 1, time, freq]
            lengths: åºåˆ—é•¿åº¦ [batch_size]
        
        Returns:
            dict: åŒ…å«åˆ†ç±»å’ŒCTCè¾“å‡ºçš„å­—å…¸
        """
        batch_size = x.size(0)
          # CNNç‰¹å¾æå–
        conv_out = self.conv_layers(x)  # [batch_size, 128, 1, time']
        conv_out = conv_out.squeeze(2).transpose(1, 2)  # [batch_size, time', 128]
        
        # RNNåºåˆ—å»ºæ¨¡
        if lengths is not None:
            # è°ƒæ•´lengthsä»¥åŒ¹é…CNNè¾“å‡ºçš„æ—¶é—´ç»´åº¦
            # CNNæœ‰ä¸¤ä¸ªMaxPool2d(2,2)ï¼Œæ‰€ä»¥æ—¶é—´ç»´åº¦è¢«ä¸‹é‡‡æ ·4å€
            adjusted_lengths = torch.clamp(lengths // 4, min=1, max=conv_out.size(1))
            conv_out = nn.utils.rnn.pack_padded_sequence(
                conv_out, adjusted_lengths.cpu(), batch_first=True, enforce_sorted=False
            )
        
        rnn_out, _ = self.rnn(conv_out)
        
        if lengths is not None:
            rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = self.attention(rnn_out)  # [batch_size, time, 1]
        attended_features = torch.sum(rnn_out * attention_weights, dim=1)  # [batch_size, hidden_dim*2]
        
        # åˆ†ç±»è¾“å‡ºï¼ˆç”¨äºæ„å›¾è¯†åˆ«ï¼‰
        class_output = self.classifier(attended_features)
        
        # CTCè¾“å‡ºï¼ˆç”¨äºè¯­éŸ³è¯†åˆ«ï¼‰
        ctc_output = self.ctc_layer(rnn_out)
        ctc_output = F.log_softmax(ctc_output, dim=-1)
        
        return {
            'classification': class_output,
            'ctc': ctc_output,
            'attention_weights': attention_weights,
            'features': attended_features
        }

class AudioFeatureExtractor:
    """
    éŸ³é¢‘ç‰¹å¾æå–å™¨
    æå–MFCCã€æ¢…å°”é¢‘è°±ç­‰ç‰¹å¾
    """
    
    def __init__(self, 
                 sample_rate: int = 16000,
                 n_mfcc: int = 80,
                 n_fft: int = 512,
                 hop_length: int = 160,
                 n_mels: int = 80):
        self.sample_rate = sample_rate
        self.n_mfcc = n_mfcc
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels
        
        # é¢„è®¡ç®—æ¢…å°”æ»¤æ³¢å™¨ç»„
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=n_fft,
            hop_length=hop_length,
            n_mels=n_mels,
            f_min=0,
            f_max=sample_rate // 2
        )
        
        self.mfcc_transform = torchaudio.transforms.MFCC(
            sample_rate=sample_rate,
            n_mfcc=n_mfcc,
            melkwargs={
                'n_fft': n_fft,
                'hop_length': hop_length,
                'n_mels': n_mels,
                'f_min': 0,
                'f_max': sample_rate // 2
            }
        )
    
    def extract_features(self, audio: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        æå–éŸ³é¢‘ç‰¹å¾
        
        Args:
            audio: éŸ³é¢‘ä¿¡å· [batch_size, samples] æˆ– [samples]
        
        Returns:
            dict: åŒ…å«å„ç§ç‰¹å¾çš„å­—å…¸
        """
        if audio.dim() == 1:
            audio = audio.unsqueeze(0)
        
        # ç¡®ä¿éŸ³é¢‘é•¿åº¦
        if audio.size(-1) < self.n_fft:
            padding = self.n_fft - audio.size(-1)
            audio = F.pad(audio, (0, padding))
        
        # æå–MFCCç‰¹å¾
        mfcc = self.mfcc_transform(audio)
        
        # æå–æ¢…å°”é¢‘è°±
        mel_spec = self.mel_transform(audio)
        mel_spec = torch.log(mel_spec + 1e-8)  # å¯¹æ•°å˜æ¢
        
        # ç‰¹å¾å½’ä¸€åŒ–
        mfcc = (mfcc - mfcc.mean(dim=-1, keepdim=True)) / (mfcc.std(dim=-1, keepdim=True) + 1e-8)
        mel_spec = (mel_spec - mel_spec.mean(dim=-1, keepdim=True)) / (mel_spec.std(dim=-1, keepdim=True) + 1e-8)
        
        return {
            'mfcc': mfcc,
            'mel_spectrogram': mel_spec,
            'raw_audio': audio
        }

class SmartHomeVocab:
    """
    æ™ºèƒ½å®¶å±…ä¸“ç”¨è¯æ±‡è¡¨
    """
    
    def __init__(self):
        self.commands = {
            # è®¾å¤‡æ§åˆ¶
            'æ‰“å¼€': 0, 'å…³é—­': 1, 'å¼€å¯': 2, 'å…³é—­': 3,
            'è°ƒèŠ‚': 4, 'è®¾ç½®': 5, 'è°ƒåˆ°': 6, 'è°ƒæ•´': 7,
            
            # è®¾å¤‡åç§°
            'ç¯': 10, 'ç”µç¯': 11, 'å°ç¯': 12, 'åŠç¯': 13,
            'ç©ºè°ƒ': 20, 'å†·æ°”': 21, 'æš–æ°”': 22,
            'ç”µè§†': 30, 'ç”µè§†æœº': 31,
            'çª—å¸˜': 40, 'ç™¾å¶çª—': 41,
            'é£æ‰‡': 50, 'åŠæ‰‡': 51,
            
            # æˆ¿é—´ä½ç½®
            'å®¢å…': 100, 'å§å®¤': 101, 'å¨æˆ¿': 102, 'ä¹¦æˆ¿': 103,
            'é˜³å°': 104, 'æ´—æ‰‹é—´': 105, 'å«ç”Ÿé—´': 106,
            
            # æ•°å€¼å’ŒçŠ¶æ€
            'ä¸€': 200, 'äºŒ': 201, 'ä¸‰': 202, 'å››': 203, 'äº”': 204,
            'ä½': 210, 'ä¸­': 211, 'é«˜': 212,
            'äº®': 220, 'æš—': 221, 'æ˜äº®': 222, 'æ˜æš—': 223,
            
            # ç‰¹æ®Šæ ‡è®°
            '<PAD>': 300, '<UNK>': 301, '<START>': 302, '<END>': 303
        }
        
        self.id_to_token = {v: k for k, v in self.commands.items()}
        self.vocab_size = len(self.commands)
        
        # é¢„å®šä¹‰çš„æ™ºèƒ½å®¶å±…æŒ‡ä»¤æ¨¡å¼
        self.command_patterns = [
            ('æ‰“å¼€å®¢å…çš„ç¯', [0, 100, 10]),
            ('å…³é—­å§å®¤çš„ç¯', [1, 101, 10]),
            ('å¼€å¯ç©ºè°ƒ', [2, 20]),
            ('è°ƒèŠ‚ç©ºè°ƒæ¸©åº¦', [4, 20]),
            ('æ‰“å¼€ç”µè§†', [0, 30]),
            ('å…³é—­çª—å¸˜', [1, 40]),
            ('å¼€å¯é£æ‰‡', [2, 50])
        ]
    
    def encode(self, text: str) -> List[int]:
        """å°†æ–‡æœ¬ç¼–ç ä¸ºtoken IDåˆ—è¡¨"""
        tokens = []
        for char in text:
            if char in self.commands:
                tokens.append(self.commands[char])
            else:
                tokens.append(self.commands['<UNK>'])
        return tokens
    
    def decode(self, token_ids: List[int]) -> str:
        """å°†token IDåˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬"""
        text = ""
        for token_id in token_ids:
            if token_id in self.id_to_token:
                text += self.id_to_token[token_id]
            else:
                text += '<UNK>'
        return text

class LightASRTrainer:
    """
    è½»é‡çº§ASRæ¨¡å‹è®­ç»ƒå™¨
    """
    
    def __init__(self, 
                 model: LightASRModel,
                 device: Optional[torch.device] = None,
                 learning_rate: float = 1e-3,
                 weight_decay: float = 1e-5):
        
        self.model = model
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        self.feature_extractor = AudioFeatureExtractor()
        self.vocab = SmartHomeVocab()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )
          # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.5, patience=5
        )
        
        # æŸå¤±å‡½æ•°
        self.ctc_loss = nn.CTCLoss(blank=self.vocab.commands['<PAD>'], reduction='mean')
        self.classification_loss = nn.CrossEntropyLoss()
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }
    
    def create_synthetic_data(self, num_samples: int = 1000) -> Tuple[List, List]:
        """
        åˆ›å»ºåˆæˆè®­ç»ƒæ•°æ®
        ç”¨äºæ¼”ç¤ºå’Œåˆæ­¥è®­ç»ƒ
        """
        print("ğŸ”§ åˆ›å»ºåˆæˆè®­ç»ƒæ•°æ®...")
        
        audio_data = []
        labels = []
        
        for i in range(num_samples):
            # ç”ŸæˆéšæœºéŸ³é¢‘ä¿¡å·ï¼ˆæ¨¡æ‹Ÿè¯­éŸ³ï¼‰
            duration = np.random.uniform(1.0, 3.0)  # 1-3ç§’
            samples = int(duration * self.feature_extractor.sample_rate)
            
            # ç”ŸæˆåŸºé¢‘å’Œè°æ³¢ï¼ˆæ¨¡æ‹Ÿäººå£°ï¼‰
            t = np.linspace(0, duration, samples)
            f0 = np.random.uniform(80, 300)  # åŸºé¢‘80-300Hz
            
            # åˆæˆè¯­éŸ³ä¿¡å·
            signal = np.zeros_like(t)
            for harmonic in range(1, 6):  # å‰5ä¸ªè°æ³¢
                amplitude = 1.0 / harmonic
                signal += amplitude * np.sin(2 * np.pi * f0 * harmonic * t)
            
            # æ·»åŠ å™ªå£°
            noise = np.random.normal(0, 0.1, samples)
            signal += noise
            
            # æ·»åŠ åŒ…ç»œ
            envelope = np.exp(-t * 2)  # æŒ‡æ•°è¡°å‡åŒ…ç»œ
            signal *= envelope
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªæŒ‡ä»¤æ¨¡å¼
            pattern = self.vocab.command_patterns[i % len(self.vocab.command_patterns)]
            command_text, token_ids = pattern
            
            audio_data.append(signal.astype(np.float32))
            labels.append({
                'text': command_text,
                'tokens': token_ids,
                'class_label': i % self.model.num_classes
            })
        
        print(f"âœ… åˆ›å»ºäº† {num_samples} ä¸ªåˆæˆæ ·æœ¬")
        return audio_data, labels
    
    def prepare_batch(self, audio_batch: List[np.ndarray], label_batch: List[Dict]) -> Tuple[torch.Tensor, Dict]:
        """
        å‡†å¤‡è®­ç»ƒæ‰¹æ¬¡
        """
        batch_features = []
        batch_lengths = []
        batch_tokens = []
        batch_classes = []
        
        for audio, label in zip(audio_batch, label_batch):
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            audio_tensor = torch.from_numpy(audio).float()
            
            # æå–ç‰¹å¾
            features = self.feature_extractor.extract_features(audio_tensor)
            mfcc = features['mfcc']  # [1, n_mfcc, time]
            
            # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å’Œé€šé“ç»´åº¦
            mfcc = mfcc.unsqueeze(0)  # [1, 1, n_mfcc, time]
            
            batch_features.append(mfcc)
            batch_lengths.append(mfcc.size(-1))
            batch_tokens.append(label['tokens'])
            batch_classes.append(label['class_label'])
        
        # å¡«å……åˆ°ç›¸åŒé•¿åº¦
        max_length = max(batch_lengths)
        padded_features = []
        
        for features in batch_features:
            if features.size(-1) < max_length:
                padding = max_length - features.size(-1)
                features = F.pad(features, (0, padding))
            padded_features.append(features)
        
        # åˆå¹¶æ‰¹æ¬¡
        batch_tensor = torch.cat(padded_features, dim=0)  # [batch_size, 1, n_mfcc, max_time]
        lengths_tensor = torch.tensor(batch_lengths)
        classes_tensor = torch.tensor(batch_classes)
        
        return batch_tensor.to(self.device), {
            'lengths': lengths_tensor.to(self.device),
            'tokens': batch_tokens,
            'classes': classes_tensor.to(self.device)
        }
    
    def train_epoch(self, audio_data: List, labels: List, batch_size: int = 16):
        """
        è®­ç»ƒä¸€ä¸ªepoch
        """
        self.model.train()
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        
        # éšæœºæ‰“ä¹±æ•°æ®
        indices = np.random.permutation(len(audio_data))
        
        for i in range(0, len(indices), batch_size):
            batch_indices = indices[i:i + batch_size]
            batch_audio = [audio_data[idx] for idx in batch_indices]
            batch_labels = [labels[idx] for idx in batch_indices]
            
            # å‡†å¤‡æ‰¹æ¬¡
            batch_features, batch_targets = self.prepare_batch(batch_audio, batch_labels)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(batch_features, batch_targets['lengths'])
            
            # è®¡ç®—æŸå¤±
            classification_loss = self.classification_loss(
                outputs['classification'], 
                batch_targets['classes']
            )
            
            # CTCæŸå¤±ï¼ˆç®€åŒ–ç‰ˆï¼Œä»…ç”¨äºæ¼”ç¤ºï¼‰
            ctc_loss = torch.tensor(0.0, device=self.device)
            
            # æ€»æŸå¤±
            loss = classification_loss + 0.1 * ctc_loss
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs['classification'], 1)
            correct = (predicted == batch_targets['classes']).sum().item()
            
            total_loss += loss.item()
            total_correct += correct
            total_samples += len(batch_labels)
        
        epoch_loss = total_loss / (len(indices) // batch_size + 1)
        epoch_acc = total_correct / total_samples
        
        return epoch_loss, epoch_acc
    
    def evaluate(self, audio_data: List, labels: List, batch_size: int = 16):
        """
        è¯„ä¼°æ¨¡å‹
        """
        self.model.eval()
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        
        with torch.no_grad():
            for i in range(0, len(audio_data), batch_size):
                batch_audio = audio_data[i:i + batch_size]
                batch_labels = labels[i:i + batch_size]
                
                batch_features, batch_targets = self.prepare_batch(batch_audio, batch_labels)
                outputs = self.model(batch_features, batch_targets['lengths'])
                
                classification_loss = self.classification_loss(
                    outputs['classification'], 
                    batch_targets['classes']
                )
                
                _, predicted = torch.max(outputs['classification'], 1)
                correct = (predicted == batch_targets['classes']).sum().item()
                
                total_loss += classification_loss.item()
                total_correct += correct
                total_samples += len(batch_labels)
        
        avg_loss = total_loss / (len(audio_data) // batch_size + 1)
        accuracy = total_correct / total_samples
        
        return avg_loss, accuracy
    
    def train(self, num_epochs: int = 50, batch_size: int = 16, validation_split: float = 0.2):
        """
        è®­ç»ƒæ¨¡å‹
        """
        print("ğŸš€ å¼€å§‹è®­ç»ƒè½»é‡çº§è¯­éŸ³è¯†åˆ«æ¨¡å‹...")
        print(f"è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        audio_data, labels = self.create_synthetic_data(num_samples=1000)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(len(audio_data) * (1 - validation_split))
        train_audio, val_audio = audio_data[:split_idx], audio_data[split_idx:]
        train_labels, val_labels = labels[:split_idx], labels[split_idx:]
        
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_audio)}, éªŒè¯æ ·æœ¬: {len(val_audio)}")
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_audio, train_labels, batch_size)
            
            # éªŒè¯
            val_loss, val_acc = self.evaluate(val_audio, val_labels, batch_size)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_loss)
            
            # è®°å½•å†å²
            self.train_history['train_loss'].append(train_loss)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['train_acc'].append(train_acc)
            self.train_history['val_acc'].append(val_acc)
            
            # æ—©åœ
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                self.save_model('best_model.pth')
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if epoch % 5 == 0:
                print(f"Epoch {epoch:3d}/{num_epochs}: "
                      f"è®­ç»ƒæŸå¤±={train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡={train_acc:.4f}, "
                      f"éªŒè¯æŸå¤±={val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡={val_acc:.4f}")
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= 10:
                print(f"æ—©åœäºç¬¬ {epoch} è½®")
                break
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        return self.train_history
    
    def save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        model_dir = os.path.dirname(os.path.abspath(__file__))
        model_path = os.path.join(model_dir, 'model', path)
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_history': self.train_history,
            'vocab': self.vocab.commands,
            'model_config': {
                'vocab_size': self.model.vocab_size,
                'input_dim': self.model.input_dim,
                'hidden_dim': self.model.hidden_dim,
                'num_layers': self.model.num_layers,
                'num_classes': self.model.num_classes
            }
        }, model_path)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

class LightASRInference:
    """
    è½»é‡çº§ASRæ¨¡å‹æ¨ç†å™¨
    """
    
    def __init__(self, model_path: Optional[str] = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.feature_extractor = AudioFeatureExtractor()
        self.vocab = SmartHomeVocab()
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            # åˆ›å»ºé»˜è®¤æ¨¡å‹
            self.model = LightASRModel(
                vocab_size=self.vocab.vocab_size,
                num_classes=50
            )
            self.model.to(self.device)
            print("âš ï¸ ä½¿ç”¨æœªè®­ç»ƒçš„æ¨¡å‹ï¼Œå»ºè®®å…ˆè¿›è¡Œè®­ç»ƒ")
    
    def load_model(self, model_path: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)
        
        model_config = checkpoint['model_config']
        self.model = LightASRModel(**model_config)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹å·²ä» {model_path} åŠ è½½")
    
    def predict(self, audio: np.ndarray) -> Dict:
        """
        å¯¹éŸ³é¢‘è¿›è¡Œé¢„æµ‹
        
        Args:
            audio: éŸ³é¢‘ä¿¡å·æ•°ç»„
        
        Returns:
            dict: é¢„æµ‹ç»“æœ
        """
        self.model.eval()
        
        with torch.no_grad():            # é¢„å¤„ç†éŸ³é¢‘
            audio_tensor = torch.from_numpy(audio).float()
            features = self.feature_extractor.extract_features(audio_tensor)
            mfcc = features['mfcc']  # [1, n_mfcc, time]
            mfcc = mfcc.unsqueeze(0)  # [1, 1, n_mfcc, time] - æ·»åŠ é€šé“ç»´åº¦
            mfcc = mfcc.to(self.device)
            
            # æ¨¡å‹æ¨ç†
            outputs = self.model(mfcc)            # åˆ†ç±»é¢„æµ‹
            class_probs = F.softmax(outputs['classification'], dim=1)
            class_pred_tensor = torch.argmax(class_probs, dim=1)
            class_pred = int(class_pred_tensor.item())
            class_conf = class_probs[0][class_pred].item()
            
            # CTCé¢„æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            ctc_probs = F.softmax(outputs['ctc'], dim=2)
            ctc_pred = torch.argmax(ctc_probs, dim=2).squeeze().cpu().numpy()
            
            return {
                'class_prediction': class_pred,
                'class_confidence': class_conf,
                'ctc_prediction': ctc_pred.tolist(),
                'attention_weights': outputs['attention_weights'].cpu().numpy(),
                'features': outputs['features'].cpu().numpy()
            }
    
    def predict_from_file(self, audio_file: str) -> Dict:
        """
        ä»éŸ³é¢‘æ–‡ä»¶è¿›è¡Œé¢„æµ‹
        
        Args:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
        Returns:
            dict: é¢„æµ‹ç»“æœ
        """
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶
        audio, sr = librosa.load(audio_file, sr=self.feature_extractor.sample_rate)
        return self.predict(audio)

def create_demo_model():
    """
    åˆ›å»ºæ¼”ç¤ºæ¨¡å‹å¹¶è¿›è¡Œç®€å•è®­ç»ƒ
    """
    print("ğŸ¯ åˆ›å»ºè½»é‡çº§è¯­éŸ³è¯†åˆ«æ¼”ç¤ºæ¨¡å‹")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡å‹
    vocab = SmartHomeVocab()
    model = LightASRModel(
        vocab_size=vocab.vocab_size,
        num_classes=50,
        hidden_dim=128,  # æ›´å°çš„éšè—å±‚ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦
        num_layers=1     # å‡å°‘å±‚æ•°ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LightASRTrainer(model, learning_rate=1e-3)
    
    # å¼€å§‹è®­ç»ƒ
    history = trainer.train(num_epochs=20, batch_size=8)
    
    # ä¿å­˜æ¨¡å‹
    trainer.save_model('light_asr_demo.pth')
    
    print("\nğŸ“Š è®­ç»ƒå†å²:")
    print(f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {history['train_acc'][-1]:.4f}")
    print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {history['val_acc'][-1]:.4f}")
    
    return model, trainer

def test_inference():
    """
    æµ‹è¯•æ¨¡å‹æ¨ç†
    """
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹æ¨ç†")
    print("=" * 30)
    
    # åˆ›å»ºæ¨ç†å™¨
    model_path = os.path.join(os.path.dirname(__file__), 'model', 'light_asr_demo.pth')
    if os.path.exists(model_path):
        inference = LightASRInference(model_path)
    else:
        print("âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨æœªè®­ç»ƒæ¨¡å‹è¿›è¡Œæ¼”ç¤º")
        inference = LightASRInference()
    
    # åˆ›å»ºæµ‹è¯•éŸ³é¢‘
    duration = 2.0
    sample_rate = 16000
    samples = int(duration * sample_rate)
    t = np.linspace(0, duration, samples)
    
    # ç”Ÿæˆæµ‹è¯•ä¿¡å·
    f0 = 150  # åŸºé¢‘
    test_audio = np.sin(2 * np.pi * f0 * t) * np.exp(-t * 2)
    test_audio += np.random.normal(0, 0.05, samples)  # æ·»åŠ å™ªå£°
    
    # è¿›è¡Œé¢„æµ‹
    result = inference.predict(test_audio.astype(np.float32))
    
    print(f"åˆ†ç±»é¢„æµ‹: {result['class_prediction']}")
    print(f"åˆ†ç±»ç½®ä¿¡åº¦: {result['class_confidence']:.4f}")
    print(f"ç‰¹å¾ç»´åº¦: {result['features'].shape}")
    print(f"æ³¨æ„åŠ›æƒé‡ç»´åº¦: {result['attention_weights'].shape}")
    
    return result

if __name__ == "__main__":
    print("ğŸ  æ™ºèƒ½å®¶å±…è½»é‡çº§è¯­éŸ³è¯†åˆ«æ¨¡å‹")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå¹¶è®­ç»ƒæ¼”ç¤ºæ¨¡å‹
        model, trainer = create_demo_model()
        
        # æµ‹è¯•æ¨ç†
        test_result = test_inference()
        
        print("\nâœ… æ¼”ç¤ºå®Œæˆ!")
        print("\nğŸ“ ä½¿ç”¨è¯´æ˜:")
        print("1. æ¨¡å‹å·²ä¿å­˜åˆ° src/train/model/ ç›®å½•")
        print("2. å¯ä»¥é€šè¿‡ LightASRInference ç±»è¿›è¡Œæ¨ç†")
        print("3. æ”¯æŒä»éŸ³é¢‘æ–‡ä»¶æˆ–numpyæ•°ç»„è¿›è¡Œé¢„æµ‹")
        print("4. æ¨¡å‹ä¸“é—¨é’ˆå¯¹æ™ºèƒ½å®¶å±…æŒ‡ä»¤è¿›è¡Œä¼˜åŒ–")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()