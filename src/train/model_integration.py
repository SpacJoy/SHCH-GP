# -*- coding: utf-8 -*-
"""
è½»é‡çº§è¯­éŸ³è¯†åˆ«æ¨¡å‹é›†æˆå™¨
å°†è‡ªè®­ç»ƒçš„è½»é‡çº§æ¨¡å‹é›†æˆåˆ°ç°æœ‰çš„è¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­
"""

import os
import sys
import numpy as np
import torch
import speech_recognition as sr
from typing import Optional, Dict, Any
import tempfile
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from train.model import LightASRInference, AudioFeatureExtractor
from config import AI_SPEECH_CONFIG, SPEECH_CONFIG

class LightASREngine:
    """
    è½»é‡çº§ASRå¼•æ“
    é›†æˆåˆ°ç°æœ‰çš„AIè¯­éŸ³è¯†åˆ«ç³»ç»Ÿä¸­
    """
    
    def __init__(self, model_path: Optional[str] = None, device: str = 'auto'):
        """
        åˆå§‹åŒ–è½»é‡çº§ASRå¼•æ“
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ('auto', 'cpu', 'cuda')
        """
        self.model_path = model_path
        
        # è®¾ç½®è®¾å¤‡
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # åˆå§‹åŒ–æ¨ç†å™¨
        self.inference = None
        self.is_loaded = False
        self.feature_extractor = AudioFeatureExtractor()
          # æ™ºèƒ½å®¶å±…æŒ‡ä»¤æ˜ å°„ - æ‰©å±•åˆ°50ä¸ªç±»åˆ«
        self.command_mapping = {
            # åŸºæœ¬ç¯å…‰æ§åˆ¶
            0: "æ‰“å¼€ç¯", 1: "å…³é—­ç¯", 2: "è°ƒäº®ç¯", 3: "è°ƒæš—ç¯",
            4: "æ‰“å¼€å®¢å…ç¯", 5: "å…³é—­å®¢å…ç¯", 6: "æ‰“å¼€å§å®¤ç¯", 7: "å…³é—­å§å®¤ç¯",
            8: "æ‰“å¼€å¨æˆ¿ç¯", 9: "å…³é—­å¨æˆ¿ç¯", 10: "æ‰“å¼€å°ç¯", 11: "å…³é—­å°ç¯",
            
            # ç©ºè°ƒæ§åˆ¶
            12: "æ‰“å¼€ç©ºè°ƒ", 13: "å…³é—­ç©ºè°ƒ", 14: "è°ƒé«˜æ¸©åº¦", 15: "è°ƒä½æ¸©åº¦",
            16: "æ‰“å¼€åˆ¶å†·", 17: "æ‰“å¼€åˆ¶çƒ­", 18: "è°ƒèŠ‚é£é€Ÿ", 19: "è®¾ç½®æ¸©åº¦",
            
            # ç”µè§†æ§åˆ¶
            20: "æ‰“å¼€ç”µè§†", 21: "å…³é—­ç”µè§†", 22: "è°ƒé«˜éŸ³é‡", 23: "è°ƒä½éŸ³é‡",
            24: "æ¢é¢‘é“", 25: "é™éŸ³", 26: "å–æ¶ˆé™éŸ³", 27: "æ’­æ”¾",
            
            # çª—å¸˜æ§åˆ¶
            28: "æ‰“å¼€çª—å¸˜", 29: "å…³é—­çª—å¸˜", 30: "æ‹‰ä¸Šçª—å¸˜", 31: "æ‹‰å¼€çª—å¸˜",
            
            # é£æ‰‡æ§åˆ¶
            32: "æ‰“å¼€é£æ‰‡", 33: "å…³é—­é£æ‰‡", 34: "è°ƒèŠ‚é£æ‰‡", 35: "é£æ‰‡åŠ é€Ÿ",
            
            # éŸ³ä¹æ§åˆ¶
            36: "æ’­æ”¾éŸ³ä¹", 37: "åœæ­¢éŸ³ä¹", 38: "æš‚åœéŸ³ä¹", 39: "ä¸‹ä¸€é¦–",
            40: "ä¸Šä¸€é¦–", 41: "éšæœºæ’­æ”¾", 42: "å¾ªç¯æ’­æ”¾", 43: "å…³é—­éŸ³ä¹",
            
            # å…¶ä»–è®¾å¤‡
            44: "æ‰“å¼€åŠ æ¹¿å™¨", 45: "å…³é—­åŠ æ¹¿å™¨", 46: "æ‰“å¼€å‡€åŒ–å™¨", 47: "å…³é—­å‡€åŒ–å™¨",
            48: "æ‰“å¼€çƒ­æ°´å™¨", 49: "å…³é—­çƒ­æ°´å™¨"
        }
        
        # å°è¯•åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        try:
            if self.model_path and os.path.exists(self.model_path):
                self.inference = LightASRInference(self.model_path)
                self.is_loaded = True
                print(f"âœ… è½»é‡çº§ASRæ¨¡å‹å·²åŠ è½½: {self.model_path}")
            else:
                # å°è¯•åŠ è½½é»˜è®¤æ¨¡å‹
                default_model_path = os.path.join(
                    os.path.dirname(__file__), 'model', 'best_model.pth'
                )
                if os.path.exists(default_model_path):
                    self.inference = LightASRInference(default_model_path)
                    self.is_loaded = True
                    print(f"âœ… å·²åŠ è½½é»˜è®¤è½»é‡çº§ASRæ¨¡å‹: {default_model_path}")
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°è½»é‡çº§ASRæ¨¡å‹ï¼Œå°†ä½¿ç”¨æœªè®­ç»ƒæ¨¡å‹")
                    self.inference = LightASRInference()
                    self.is_loaded = False
        except Exception as e:
            print(f"âŒ è½»é‡çº§ASRæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.inference = None
            self.is_loaded = False
    
    def recognize(self, audio: sr.AudioData) -> Optional[str]:
        """
        è¯†åˆ«è¯­éŸ³
        
        Args:
            audio: è¯­éŸ³æ•°æ®
        
        Returns:
            Optional[str]: è¯†åˆ«ç»“æœæ–‡æœ¬
        """
        if not self.inference:
            return None
        
        try:
            # è½¬æ¢éŸ³é¢‘æ ¼å¼
            wav_data = audio.get_wav_data()
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                tmp_file.write(wav_data)
                tmp_path = tmp_file.name
            
            try:                # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
                result = self.inference.predict_from_file(tmp_path)
                
                # è·å–é¢„æµ‹ç»“æœ
                class_pred = result['class_prediction']
                confidence = result['class_confidence']
                
                # æ˜ å°„åˆ°æ–‡æœ¬ - é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ç”¨äºæµ‹è¯•
                if class_pred in self.command_mapping and confidence > 0.02:
                    recognized_text = self.command_mapping[class_pred]
                    return recognized_text
                else:
                    return None
                    
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(tmp_path)
                
        except Exception as e:
            print(f"âŒ è½»é‡çº§ASRè¯†åˆ«å¤±è´¥: {e}")
            return None

    def recognize_file(self, file_path: str) -> Optional[str]:
        """
        ä»éŸ³é¢‘æ–‡ä»¶è¯†åˆ«è¯­éŸ³
        
        Args:
            file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
        Returns:
            Optional[str]: è¯†åˆ«ç»“æœæ–‡æœ¬
        """
        if not self.inference:
            return None
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None
              # ç›´æ¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
            result = self.inference.predict_from_file(file_path)
            
            # è·å–é¢„æµ‹ç»“æœ
            class_pred = result['class_prediction']
            confidence = result['class_confidence']
            
            # æ˜ å°„åˆ°æ–‡æœ¬ - é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ç”¨äºæµ‹è¯•
            if class_pred in self.command_mapping and confidence > 0.02:
                recognized_text = self.command_mapping[class_pred]
                return recognized_text
            else:
                return None
                
        except Exception as e:
            print(f"âŒ è½»é‡çº§ASRæ–‡ä»¶è¯†åˆ«å¤±è´¥: {e}")
            return None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥å¼•æ“æ˜¯å¦å¯ç”¨"""
        return self.inference is not None
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if not self.inference:
            return {"status": "not_loaded"}
        
        return {
            "status": "loaded" if self.is_loaded else "default",
            "device": str(self.device),
            "model_path": self.model_path,
            "commands": list(self.command_mapping.values())
        }

class EnhancedAISpeechRecognizer:
    """
    å¢å¼ºçš„AIè¯­éŸ³è¯†åˆ«å™¨
    é›†æˆè½»é‡çº§æœ¬åœ°æ¨¡å‹
    """
    
    def __init__(self, status_callback=None, use_light_asr=True):
        """
        åˆå§‹åŒ–å¢å¼ºè¯­éŸ³è¯†åˆ«å™¨
        
        Args:
            status_callback: çŠ¶æ€å›è°ƒå‡½æ•°
            use_light_asr: æ˜¯å¦ä½¿ç”¨è½»é‡çº§ASR
        """
        # å¯¼å…¥åŸæœ‰çš„AIè¯­éŸ³è¯†åˆ«å™¨
        from ai_speech_recognition import AISpeechRecognizer
        
        # åˆå§‹åŒ–åŸºç¡€è¯†åˆ«å™¨
        self.base_recognizer = AISpeechRecognizer(status_callback)
        self.status_callback = status_callback
        
        # åˆå§‹åŒ–è½»é‡çº§ASRå¼•æ“
        self.light_asr = None
        if use_light_asr:
            try:
                self.light_asr = LightASREngine()
                if self.light_asr.is_available():
                    self._update_status("âœ… è½»é‡çº§ASRå¼•æ“å·²å¯ç”¨")
                else:
                    self._update_status("âš ï¸ è½»é‡çº§ASRå¼•æ“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†å¼•æ“")
            except Exception as e:
                self._update_status(f"âš ï¸ è½»é‡çº§ASRåˆå§‹åŒ–å¤±è´¥: {e}")
                self.light_asr = None
        
        # è¯†åˆ«ç­–ç•¥é…ç½®
        self.recognition_strategy = AI_SPEECH_CONFIG.get('strategy', 'hybrid')
        # å¯é€‰: 'light_first', 'standard_first', 'hybrid', 'light_only', 'standard_only'
    
    def _update_status(self, message: str):
        """æ›´æ–°çŠ¶æ€"""
        if self.status_callback:
            self.status_callback(message)
        print(f"[Enhanced ASR] {message}")
    
    def _recognize_with_light_asr(self, audio: sr.AudioData) -> Optional[str]:
        """ä½¿ç”¨è½»é‡çº§ASRè¯†åˆ«"""
        if not self.light_asr or not self.light_asr.is_available():
            return None
        
        try:
            self._update_status("ğŸ”¬ å°è¯•è½»é‡çº§ASRè¯†åˆ«...")
            result = self.light_asr.recognize(audio)
            if result:
                self._update_status(f"âœ… è½»é‡çº§ASRè¯†åˆ«æˆåŠŸ: {result}")
                return result
            else:
                self._update_status("âš ï¸ è½»é‡çº§ASRæœªè¯†åˆ«åˆ°å†…å®¹")
                return None
        except Exception as e:
            self._update_status(f"âŒ è½»é‡çº§ASRè¯†åˆ«å¼‚å¸¸: {e}")
            return None
    
    def _recognize_with_standard_asr(self, audio: sr.AudioData) -> Optional[str]:
        """ä½¿ç”¨æ ‡å‡†ASRè¯†åˆ«"""
        try:
            self._update_status("ğŸŒ ä½¿ç”¨æ ‡å‡†ASRè¯†åˆ«...")
            result = self.base_recognizer._recognize_with_ai(audio)
            if result:
                self._update_status(f"âœ… æ ‡å‡†ASRè¯†åˆ«æˆåŠŸ: {result}")
                return result
            else:
                self._update_status("âš ï¸ æ ‡å‡†ASRæœªè¯†åˆ«åˆ°å†…å®¹")
                return None
        except Exception as e:
            self._update_status(f"âŒ æ ‡å‡†ASRè¯†åˆ«å¼‚å¸¸: {e}")
            return None
    
    def recognize_with_strategy(self, audio: sr.AudioData) -> Optional[str]:
        """
        æ ¹æ®ç­–ç•¥è¿›è¡Œè¯†åˆ«
        
        Args:
            audio: éŸ³é¢‘æ•°æ®
        
        Returns:
            Optional[str]: è¯†åˆ«ç»“æœ
        """
        if self.recognition_strategy == 'light_only':
            # ä»…ä½¿ç”¨è½»é‡çº§ASR
            return self._recognize_with_light_asr(audio)
            
        elif self.recognition_strategy == 'standard_only':
            # ä»…ä½¿ç”¨æ ‡å‡†ASR
            return self._recognize_with_standard_asr(audio)
            
        elif self.recognition_strategy == 'light_first':
            # ä¼˜å…ˆä½¿ç”¨è½»é‡çº§ASRï¼Œå¤±è´¥åä½¿ç”¨æ ‡å‡†ASR
            result = self._recognize_with_light_asr(audio)
            if result:
                return result
            return self._recognize_with_standard_asr(audio)
            
        elif self.recognition_strategy == 'standard_first':
            # ä¼˜å…ˆä½¿ç”¨æ ‡å‡†ASRï¼Œå¤±è´¥åä½¿ç”¨è½»é‡çº§ASR
            result = self._recognize_with_standard_asr(audio)
            if result:
                return result
            return self._recognize_with_light_asr(audio)
            
        elif self.recognition_strategy == 'hybrid':
            # æ··åˆç­–ç•¥ï¼šå¯¹äºç®€å•æŒ‡ä»¤ä½¿ç”¨è½»é‡çº§ï¼Œå¤æ‚æŒ‡ä»¤ä½¿ç”¨æ ‡å‡†
            # å…ˆå°è¯•è½»é‡çº§ASR
            light_result = self._recognize_with_light_asr(audio)
            if light_result:
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ™ºèƒ½å®¶å±…æŒ‡ä»¤
                smart_home_keywords = ['ç¯', 'ç©ºè°ƒ', 'ç”µè§†', 'çª—å¸˜', 'é£æ‰‡', 'æ‰“å¼€', 'å…³é—­']
                if any(keyword in light_result for keyword in smart_home_keywords):
                    self._update_status("ğŸ¯ æ£€æµ‹åˆ°æ™ºèƒ½å®¶å±…æŒ‡ä»¤ï¼Œä½¿ç”¨è½»é‡çº§ASRç»“æœ")
                    return light_result
            
            # ä½¿ç”¨æ ‡å‡†ASRè¿›è¡Œæ›´å‡†ç¡®çš„è¯†åˆ«
            standard_result = self._recognize_with_standard_asr(audio)
            if standard_result:
                return standard_result
                
            # å¦‚æœæ ‡å‡†ASRä¹Ÿå¤±è´¥ï¼Œè¿”å›è½»é‡çº§ASRçš„ç»“æœ
            return light_result
        
        else:
            # é»˜è®¤ç­–ç•¥
            return self._recognize_with_standard_asr(audio)
    
    def recognize_once(self, timeout: int = 5) -> Optional[str]:
        """
        å•æ¬¡è¯­éŸ³è¯†åˆ«
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´
        
        Returns:
            Optional[str]: è¯†åˆ«ç»“æœ
        """
        try:
            self._update_status("ğŸ¤ å¼€å§‹å¢å¼ºè¯­éŸ³è¯†åˆ«...")
            
            # å½•éŸ³
            with self.base_recognizer.microphone as source:
                audio = self.base_recognizer.recognizer.listen(source, timeout=timeout, phrase_time_limit=5)
            
            # ä½¿ç”¨ç­–ç•¥è¿›è¡Œè¯†åˆ«
            result = self.recognize_with_strategy(audio)
            
            if result:
                self._update_status(f"âœ… è¯†åˆ«å®Œæˆ: {result}")
                return result
            else:
                self._update_status("âŒ è¯†åˆ«å¤±è´¥")
                return None
                
        except sr.WaitTimeoutError:
            self._update_status("â° å½•éŸ³è¶…æ—¶")
            return None
        except Exception as e:
            self._update_status(f"âŒ è¯†åˆ«å¼‚å¸¸: {e}")
            return None
    
    def listen_continuous(self) -> Optional[str]:
        """æŒç»­ç›‘å¬ï¼ˆå•æ¬¡è°ƒç”¨ï¼‰"""
        return self.base_recognizer.listen_continuous()
    
    def listen_with_wake_word(self) -> Optional[str]:
        """å¸¦å”¤é†’è¯çš„ç›‘å¬"""
        return self.base_recognizer.listen_with_wake_word()
    
    def is_microphone_available(self) -> bool:
        """æ£€æŸ¥éº¦å…‹é£æ˜¯å¦å¯ç”¨"""
        return self.base_recognizer.is_microphone_available()
    
    def get_engine_status(self) -> Dict[str, Any]:
        """è·å–å¼•æ“çŠ¶æ€"""
        status = {
            'base_engine': self.base_recognizer.engine,
            'light_asr_available': self.light_asr.is_available() if self.light_asr else False,
            'recognition_strategy': self.recognition_strategy
        }
        
        if self.light_asr:
            status['light_asr_info'] = self.light_asr.get_model_info()
        
        return status

def integrate_light_asr():
    """
    é›†æˆè½»é‡çº§ASRåˆ°ç°æœ‰ç³»ç»Ÿ
    """
    print("ğŸ”— é›†æˆè½»é‡çº§ASRåˆ°ç°æœ‰è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ")
    print("=" * 50)
    
    # ä¿®æ”¹AIè¯­éŸ³è¯†åˆ«é…ç½®
    config_updates = {
        'light_asr': {
            'enabled': True,
            'model_path': None,  # ä½¿ç”¨é»˜è®¤æ¨¡å‹
            'strategy': 'hybrid',  # æ··åˆç­–ç•¥
            'fallback_to_standard': True
        }
    }
    
    # æ›´æ–°é…ç½®æ–‡ä»¶
    config_file = os.path.join(os.path.dirname(__file__), '..', 'config.py')
    
    print(f"ğŸ“ å»ºè®®é…ç½®æ›´æ–°:")
    print("åœ¨ config.py ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®:")
    print(f"LIGHT_ASR_CONFIG = {config_updates}")
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    try:
        enhanced_recognizer = EnhancedAISpeechRecognizer()
        status = enhanced_recognizer.get_engine_status()
        
        print(f"\nğŸ“Š å¼•æ“çŠ¶æ€:")
        print(f"åŸºç¡€å¼•æ“: {status['base_engine']}")
        print(f"è½»é‡çº§ASRå¯ç”¨: {status['light_asr_available']}")
        print(f"è¯†åˆ«ç­–ç•¥: {status['recognition_strategy']}")
        
        if status['light_asr_available']:
            light_info = status['light_asr_info']
            print(f"è½»é‡çº§ASRçŠ¶æ€: {light_info['status']}")
            print(f"æ”¯æŒæŒ‡ä»¤æ•°: {len(light_info['commands'])}")
        
        print("\nâœ… é›†æˆå®Œæˆ!")
        return enhanced_recognizer
        
    except Exception as e:
        print(f"âŒ é›†æˆå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    # é›†æˆæµ‹è¯•
    enhanced_recognizer = integrate_light_asr()
    
    if enhanced_recognizer:
        print("\nğŸ§ª è¿è¡Œé›†æˆæµ‹è¯•...")
        
        # æµ‹è¯•éº¦å…‹é£
        if enhanced_recognizer.is_microphone_available():
            print("ğŸ¤ éº¦å…‹é£å¯ç”¨")
            
            # å¯ä»¥æ·»åŠ å®é™…çš„è¯­éŸ³è¯†åˆ«æµ‹è¯•
            # result = enhanced_recognizer.recognize_once(timeout=10)
            # print(f"è¯†åˆ«ç»“æœ: {result}")
        else:
            print("âŒ éº¦å…‹é£ä¸å¯ç”¨")
        
        print("âœ… é›†æˆæµ‹è¯•å®Œæˆ")
    else:
        print("âŒ é›†æˆæµ‹è¯•å¤±è´¥")
