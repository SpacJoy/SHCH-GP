#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è°ƒè¯•è„šæœ¬
æ£€æŸ¥æ¨¡å‹çš„å®é™…è¡Œä¸ºå’Œè¾“å‡º
"""

import os
import sys
import torch
import numpy as np
import traceback

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from model_integration import LightASREngine
from dataset_generator import SmartHomeSpeechSynthesizer

def debug_model_output():
    """è°ƒè¯•æ¨¡å‹è¾“å‡º"""
    print("ğŸ” å¼€å§‹æ¨¡å‹è°ƒè¯•...")
    print("=" * 50)
    
    # åˆå§‹åŒ–å¼•æ“
    engine = LightASREngine()
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
    print(f"æ¨¡å‹æ˜¯å¦å¯ç”¨: {engine.is_available()}")
    print(f"æ¨¡å‹ä¿¡æ¯: {engine.get_model_info()}")
    
    # ç”Ÿæˆä¸€ä¸ªç®€å•çš„æµ‹è¯•éŸ³é¢‘
    print("\nğŸµ ç”Ÿæˆæµ‹è¯•éŸ³é¢‘...")
    synthesizer = SmartHomeSpeechSynthesizer()
    test_command = "æ‰“å¼€å®¢å…çš„ç¯"
    
    try:
        # ç”ŸæˆéŸ³é¢‘
        audio = synthesizer.synthesize_speech(test_command, duration=2.0)
        print(f"éŸ³é¢‘ç”ŸæˆæˆåŠŸï¼Œå½¢çŠ¶: {audio.shape}")
        
        # ä¿å­˜ä¸ºæ–‡ä»¶
        import torchaudio
        audio_tensor = torch.from_numpy(audio).unsqueeze(0)
        test_file = "debug_test.wav"
        torchaudio.save(test_file, audio_tensor, synthesizer.sample_rate)
        print(f"éŸ³é¢‘å·²ä¿å­˜åˆ°: {test_file}")
        
        # ç›´æ¥æµ‹è¯•æ¨ç†å™¨
        print("\nğŸš€ æµ‹è¯•æ¨ç†å™¨...")
        if engine.inference:
            result = engine.inference.predict_from_file(test_file)
            print(f"æ¨ç†ç»“æœ: {result}")
            
            # æ£€æŸ¥å„ä¸ªç»„ä»¶
            class_pred = result.get('class_prediction', None)
            confidence = result.get('class_confidence', None)
            
            print(f"ç±»åˆ«é¢„æµ‹: {class_pred}")
            print(f"ç½®ä¿¡åº¦: {confidence}")
            
            # æ£€æŸ¥å‘½ä»¤æ˜ å°„
            if class_pred is not None and class_pred in engine.command_mapping:
                mapped_command = engine.command_mapping[class_pred]
                print(f"æ˜ å°„çš„å‘½ä»¤: {mapped_command}")
            else:
                print("å‘½ä»¤æ˜ å°„å¤±è´¥")
                print(f"å¯ç”¨æ˜ å°„: {list(engine.command_mapping.keys())}")
        
        # æµ‹è¯•å®Œæ•´çš„recognize_fileæ–¹æ³•
        print("\nğŸ¯ æµ‹è¯•å®Œæ•´è¯†åˆ«æ–¹æ³•...")
        final_result = engine.recognize_file(test_file)
        print(f"æœ€ç»ˆè¯†åˆ«ç»“æœ: {final_result}")
        
        # æ¸…ç†
        if os.path.exists(test_file):
            os.remove(test_file)
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()

def debug_audio_processing():
    """è°ƒè¯•éŸ³é¢‘å¤„ç†è¿‡ç¨‹"""
    print("\nğŸ”§ è°ƒè¯•éŸ³é¢‘å¤„ç†...")
    print("=" * 50)
    
    # ç›´æ¥æµ‹è¯•ç‰¹å¾æå–
    from model import AudioFeatureExtractor, LightASRInference
    
    feature_extractor = AudioFeatureExtractor()
    
    # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘
    synthesizer = SmartHomeSpeechSynthesizer()
    audio = synthesizer.synthesize_speech("æ‰“å¼€ç¯", duration=2.0)
    
    print(f"åŸå§‹éŸ³é¢‘å½¢çŠ¶: {audio.shape}")
    print(f"éŸ³é¢‘é‡‡æ ·ç‡: {synthesizer.sample_rate}")
    
    # è½¬æ¢ä¸ºtensor
    audio_tensor = torch.from_numpy(audio).float()
    print(f"éŸ³é¢‘tensorå½¢çŠ¶: {audio_tensor.shape}")
    
    # æå–ç‰¹å¾
    try:
        features = feature_extractor.extract_features(audio_tensor)
        print(f"MFCCç‰¹å¾å½¢çŠ¶: {features['mfcc'].shape}")
        print(f"æ¢…å°”é¢‘è°±å½¢çŠ¶: {features['mel_spectrogram'].shape}")
    except Exception as e:
        print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
        traceback.print_exc()

def debug_model_weights():
    """è°ƒè¯•æ¨¡å‹æƒé‡"""
    print("\nâš–ï¸ è°ƒè¯•æ¨¡å‹æƒé‡...")
    print("=" * 50)
    
    engine = LightASREngine()
    
    if engine.inference and engine.inference.model:
        model = engine.inference.model
        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"æ€»å‚æ•°æ•°é‡: {total_params}")
        print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params}")
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        print(f"æ¨¡å‹è®­ç»ƒæ¨¡å¼: {model.training}")
        
        # æ£€æŸ¥è®¾å¤‡
        device = next(model.parameters()).device
        print(f"æ¨¡å‹è®¾å¤‡: {device}")
        
        # æ£€æŸ¥åˆ†ç±»å™¨æƒé‡
        classifier_weight = model.classifier[0].weight
        print(f"åˆ†ç±»å™¨æƒé‡å½¢çŠ¶: {classifier_weight.shape}")
        print(f"åˆ†ç±»å™¨æƒé‡èŒƒå›´: [{classifier_weight.min():.3f}, {classifier_weight.max():.3f}]")

if __name__ == "__main__":
    debug_model_output()
    debug_audio_processing()
    debug_model_weights()
